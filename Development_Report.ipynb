{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPC2jbKE3+luK6lABM/1joW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhavini30/IBP-Assigment-2/blob/main/Development_Report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgJega-n4hgu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For the development of the business report:**\n",
        "\n",
        "**Accessing The  Enron Database**\n",
        "\n",
        "Firstly we had to run the !wget code provided to us to download enron on google colab.\n",
        "\n",
        "!wget -O enron.db https://curtin-my.sharepoint.com/:u:/g/personal/211934g_curtin_edu_au/EaYagsqa2r1Bi5wtHbswGFwBH2kd2uTnz6rlka7GI36GUQ?download=1\n",
        " \n",
        " **Establishing connection to the database with SQLite library**\n",
        "\n",
        "Then we had to import the sqlite3 library and establish a connection with the database with the variable \"conn\"\n",
        "import sqlite\n",
        "\n",
        "conn =sqlite3.connect(\"enron.db\")\n",
        "\n",
        "**Creating a Cursor Object**\n",
        "\n",
        "We also had to create a cursor object to help in executing the sql queries.\n"
      ],
      "metadata": {
        "id": "717_DevK49_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Check Schema of  Enron DataBase**\n",
        "\n",
        "Firstly, we had to check the schema of the database.\n",
        "\n",
        "To do that, we use an SQL query to retrieve all the tables in the database.\n",
        "\n",
        "**Check the Schema of all the Tables Found In the Database**\n",
        "\n",
        "From the result of the previous query, we then had to check the schema of all the tables present in the database. Firstly we checked for the 'employeelist' table, secondly the 'message' table, thirdly the 'recipentinfo' table and finally, the 'referenceinfo' table.\n",
        "\n",
        "\n",
        "**Data Extraction and Manipulation**\n",
        "\n",
        "\n",
        "**Using Pandas to Convert query results into DataFrame**\n",
        "\n",
        "Firstly, we must import the pandas library to convert the data inside the table into DataFrame.\n",
        "\n",
        "import pandas\n",
        " \n",
        " **Checking Format of data**\n",
        "\n",
        "We then had to use an SQL query to check the date field format in the message table.\n",
        "\n",
        "**Fetching Records from the message Table**\n",
        "\n",
        "We also had to write an SQL query to fetch the records from the message table and use the pandas to create a DataFrame named 'message_pd' to store the records.Then we display the records.\n",
        "\n",
        "**Checking for null values**\n",
        "\n",
        "After displaying the records,we wrote a code to check to see it there is any null values present in the 'message_df\n"
      ],
      "metadata": {
        "id": "-QEzvl2kZAAG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analyses and Visualisations to Gain Insight inthe Enron's Coomunication and\n",
        "Organisation Structure**\n"
      ],
      "metadata": {
        "id": "7xMiY2MFXIwf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Email Traffic Over Time**\n",
        "\n",
        "For the first part of the analysis,we had to write an SQL code to count the number of emails received: daily, monthly, and weekly.\n",
        "\n",
        "\n",
        "For each SQL query, we then had to convert the results from the queries into DataFrames\n",
        "\n",
        "\n",
        "Then we imported the matplotlib database and the seaborn library and used the respective DataFrames to plot a graph for each SQL query. \n",
        "\n",
        "\n",
        "Then, for each graph plotted, we had to do an analysis based on the outputted data."
      ],
      "metadata": {
        "id": "R9w3uMtnTJJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Top Senders and Recipients**\n",
        "\n",
        "For the second part of the analysis, we have to use an sql query to find the sender and receiver of emails with the highest email count.\n",
        "\n",
        "To do that, we first used an SQL query to find the data of the highest sender and receiver respectively and limit the output to 10 results. Secondly,we had to convert the data into a DataFrame.\n",
        "\n",
        "Finally, we used the matplotlib, seaborn and the DataFrame to plot Barchart for The highest sender and receiver.We then analysed the results of both bar charts"
      ],
      "metadata": {
        "id": "DREe-atnTIcW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Email Distribution By Recipient Type**\n",
        "\n",
        "Finally,we had to write an SQL query to count the emails for each recipient type(rtype). Then we have to convert the data into a DataFrame and plot a piechart using the data from the DataFrame, and we also used the seaborn to change the colour scheme of the Piechart."
      ],
      "metadata": {
        "id": "WpTdBSsjR5md"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing**\n",
        "\n",
        "While running the codes, we encountered some difficulties in trying to include the Seaborn library into our code for the graph, and after consulting with our tutor, we were able to solve the issues.\n",
        "\n",
        "Another issue I had while running my codes was that, at first, the name of my DataFrame was similar; due to that, the data stored in the DataFrame was not the one I was trying to interpret. Therefore I had to change the name of the DataFrame."
      ],
      "metadata": {
        "id": "7iRwOLCjK2Gk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Industry Best Pratice**\n",
        "\n",
        "The industry best practice that was not applied for this assignment was getting our code reviewed by a team of developers so that they could evaluate the quality of the code and identify issues that could arise.\n",
        "\n",
        "We could not apply this practice as the code was for an assignment; therefore we had to review our code ourselves.\n",
        "\n",
        "Another industry best pratice  that we could not applied for this assignment They Consist of the Extensive Testing Framework whereby codes are usually tested through many libraries and frameworks.\n",
        "\n",
        "The reason why we were not able to apply this was that the data provided for the assignment did not require extensive testing to be done."
      ],
      "metadata": {
        "id": "JOhad9UnOBbT"
      }
    }
  ]
}